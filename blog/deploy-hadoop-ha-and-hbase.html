<!DOCTYPE html>
<html lang="zh-CN">
  <head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <link rel="stylesheet" href="/assets/css/style.css">
  <link rel="stylesheet" href="/assets/css/syntax.css">

  <link rel="shortcut icon" href="/assets/images/logo.png " type="image/png">
  <link rel="icon" href="/assets/images/logo.png " type="image/png">

  <!-- Begin Jekyll SEO tag v2.8.0 -->
<title>部署 Hadoop HA 和 HBase 集群 | Hugo Chou 周昌权</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="部署 Hadoop HA 和 HBase 集群" />
<meta name="author" content="Hugo Chou" />
<meta property="og:locale" content="zh_CN" />
<meta name="description" content="简介" />
<meta property="og:description" content="简介" />
<link rel="canonical" href="https://hugochougt.com/blog/deploy-hadoop-ha-and-hbase" />
<meta property="og:url" content="https://hugochougt.com/blog/deploy-hadoop-ha-and-hbase" />
<meta property="og:site_name" content="Hugo Chou 周昌权" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2013-09-11T21:33:00+00:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="部署 Hadoop HA 和 HBase 集群" />
<meta name="twitter:site" content="@hugochougt" />
<meta name="twitter:creator" content="@hugochougt" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Hugo Chou","url":"https://hugochougt.com"},"dateModified":"2013-09-11T21:33:00+00:00","datePublished":"2013-09-11T21:33:00+00:00","description":"简介","headline":"部署 Hadoop HA 和 HBase 集群","mainEntityOfPage":{"@type":"WebPage","@id":"https://hugochougt.com/blog/deploy-hadoop-ha-and-hbase"},"publisher":{"@type":"Organization","logo":{"@type":"ImageObject","url":"https://hugochougt.com/assets/images/logo.png"},"name":"Hugo Chou"},"url":"https://hugochougt.com/blog/deploy-hadoop-ha-and-hbase"}</script>
<!-- End Jekyll SEO tag -->

</head>

  <body>
    <nav class="flex flex-wrap items-center justify-center px-2 py-6">
  <div class="container px-4 mx-auto flex flex-wrap items-center justify-center">
    <div class="flex md:block" id="navbar">
      <div class="flex mr-auto w-full">
        
        <a class="text-sm text-gray-500 hover:text-gray-900 px-3 py-2 lg:py-1 mx-auto uppercase"
          href="/">
          Home
        </a>
        
        <a class="text-sm text-gray-500 hover:text-gray-900 px-3 py-2 lg:py-1 mx-auto uppercase"
          href="/blog/">
          Blog
        </a>
        
      </div>
    </div>
  </div>
</nav>


<div class="flex flex-wrap items-center justify-center mx-auto px-4 pt-4 prose prose-stone">
  <h1 class="text-center mt-6 mb-6">部署 Hadoop HA 和 HBase 集群</h1>
  <article class="py-1">
    <h2 id="简介">简介</h2>

<p>本文主要介绍在局域网中使用 tarball 手工配置和部署 Hadoop 高可用性集群和 HBase。使用的是 cloudera 公司的 CDH 4.2.0 版本，需要下载 hadoop、HBase 和 ZooKeeper 三个压缩包。</p>

<h2 id="集群-ip-及角色分配">集群 IP 及角色分配</h2>

<ul>
  <li>172.16.2.141 – NameNode, DataNode, NodeManager, JournalNode, ZooKeeper</li>
  <li>172.16.2.142 – NameNode, DataNode, NodeManager, JournalNode, ZooKeeper</li>
  <li>172.16.2.143 – ResourceManager, DataNode, NodeManager, JournalNode, ZooKeeper, HRegionServer</li>
  <li>172.16.2.144 – DataNode, NodeManager, HRegionServer</li>
  <li>172.16.2.145 – DataNode, NodeManager, HRegionServer</li>
</ul>

<h2 id="准备工作">准备工作</h2>

<h3 id="安装-ssh-并启动-sshd-服务">安装 ssh 并启动 sshd 服务</h3>

<p>为集群的机器安装操作系统时（后），一定安装好 ssh 并启动 sshd 服务，不然集群无法通信。</p>

<h3 id="创建集群的用户组和用户">创建集群的用户组和用户</h3>

<p>分别在集群中的每台机器创建一样的用户组和用户，比如用户组和用户名均为 hadoop。可通过 <code class="language-plaintext highlighter-rouge">groupadd</code> 和 <code class="language-plaintext highlighter-rouge">useradd</code> 两个命令来创建，如下：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>groupadd hadoop
useradd <span class="nt">-g</span> hadoop <span class="nt">-m</span> /home/hadoop hadoop
</code></pre></div></div>

<h3 id="ssh-无密码登录">ssh 无密码登录</h3>

<p>为集群中的 hadoop 用户配置 ssh 无密码登录，具体做法请自行 google。</p>

<h3 id="添加-ip-对主机名的映射关系">添加 IP 对主机名的映射关系</h3>

<p>编辑 /etc/hosts 文件，为集群中的每一个 IP 添加一个对应的主机名。例如：</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>172.16.2.141 node1
172.16.2.142 node2
172.16.2.143 node3
172.16.2.144 node4
172.16.2.145 node5
</code></pre></div></div>

<h3 id="安装-jdk-16-or-later">安装 JDK 1.6 (or later)</h3>

<p>怎样安装 JDK 就不废话了，能用上 Hadoop 的，一般都有 Java 开发环境的配置经验。</p>

<h3 id="同步整个集群的机器时间">同步整个集群的机器时间</h3>

<p>时间同步是 HBase 要求的，可通过 <code class="language-plaintext highlighter-rouge">sntp</code> 或者 <code class="language-plaintext highlighter-rouge">date -s</code> 命令来同步整个集群的时间。时间相差太多的话 HBase 会无法启动成功的。</p>

<h2 id="部署-zookeeper">部署 ZooKeeper</h2>

<p>本方案部署3个节点的 ZooKeeper，分别部署在 node1、node2、node3 三台机器上。ZooKeeper 部署的节点数建议为奇数个。</p>

<h3 id="解压-zookeeper-压缩包">解压 ZooKeeper 压缩包</h3>

<p>cd 进入安装目录（本文所有安装目录默认在 $HOME 下），使用下面的命令即可解压文件。解压 Hadoop 和 HBase 的 tarball 一样。</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">tar</span> <span class="nt">-xzvf</span> zookeeper-3.4.5-cdh4.2.0.tar.gz
</code></pre></div></div>

<h3 id="配置-zoocfg">配置 zoo.cfg</h3>

<p>在 zookeeper-3.4.5-cdh4.2.0/conf 目录下创建 zoo.cfg 文件，并添加以下内容：</p>

<div class="language-config highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">tickTime</span>=<span class="m">2000</span>
<span class="n">initLimit</span>=<span class="m">5</span>
<span class="n">syncLimit</span>=<span class="m">2</span>
<span class="n">dataDir</span>=/<span class="n">home</span>/<span class="n">hadoop</span>/<span class="n">data</span>/<span class="n">zookeeper</span>/<span class="n">data</span>
<span class="n">dataLogDir</span>=/<span class="n">home</span>/<span class="n">hadoop</span>/<span class="n">data</span>/<span class="n">zookeeper</span>/<span class="n">logs</span>
<span class="n">clientPort</span>=<span class="m">2181</span>

<span class="n">server</span>.<span class="m">1</span>=<span class="m">172</span>.<span class="m">16</span>.<span class="m">2</span>.<span class="m">141</span>:<span class="m">2888</span>:<span class="m">3888</span>
<span class="n">server</span>.<span class="m">2</span>=<span class="m">172</span>.<span class="m">16</span>.<span class="m">2</span>.<span class="m">142</span>:<span class="m">2888</span>:<span class="m">3888</span>
<span class="n">server</span>.<span class="m">3</span>=<span class="m">172</span>.<span class="m">16</span>.<span class="m">2</span>.<span class="m">143</span>:<span class="m">2888</span>:<span class="m">3888</span>
</code></pre></div></div>

<p>然后在每台部署 ZooKeeper 服务的机器上创建 <code class="language-plaintext highlighter-rouge">dataDir</code> 和 <code class="language-plaintext highlighter-rouge">dataLogDir</code> 两个目录，并在 <code class="language-plaintext highlighter-rouge">dataDir</code> 目录中创建名为 “myid” 的文件，在 server.1 的 myid 文件中写入”1”这个数字，在 server.2 的 myid 文件写入”2”， server.3 写”3”，以此类推。</p>

<h3 id="启动-zookeeper-服务">启动 ZooKeeper 服务</h3>

<p>分别 ssh 登录到每台机器上，cd 进 zookeeper-3.4.5-cdh4.2.0/bin 目录，执行 <code class="language-plaintext highlighter-rouge">zkServer.sh start</code> 命令来启动 ZooKeeper 服务：</p>

<h3 id="测试">测试</h3>

<p>ssh 登录到某台 ZooKeeper 的机器，在 zookeeper-3.4.5-cdh4.2.0/bin 目录，运行 <code class="language-plaintext highlighter-rouge">./zkCli.sh -server 172.16.2.141:2181</code> 命令来启动一个 ZooKeeper 客户端。在客户端输入 <code class="language-plaintext highlighter-rouge">help</code> 查看帮助，并执行一些简单操作来检查 ZooKeeper 的服务是否正常。</p>

<h2 id="部署-hadoop">部署 Hadoop</h2>

<h3 id="解压-hadoop-压缩包">解压 Hadoop 压缩包</h3>

<h3 id="配置-bashrc-文件">配置 ~/.bashrc 文件</h3>

<p>编辑集群中每台机器的 ~/.bashrc 文件，添加以下 Hadoop 的环境变量：</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">export </span><span class="nv">HADOOP_PREFIX</span><span class="o">=</span>/home/hadoop/hadoop-2.0.0-cdh4.2.0
<span class="nb">export </span><span class="nv">HADOOP_HOME</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_PREFIX</span><span class="k">}</span>
<span class="nb">export </span><span class="nv">HADOOP_MAPRED_HOME</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_PREFIX</span><span class="k">}</span>
<span class="nb">export </span><span class="nv">HADOOP_COMMON_HOME</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_PREFIX</span><span class="k">}</span>
<span class="nb">export </span><span class="nv">HADOOP_HDFS_HOME</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_PREFIX</span><span class="k">}</span>
<span class="nb">export </span><span class="nv">YARN_HOME</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_PREFIX</span><span class="k">}</span>
<span class="nb">export </span><span class="nv">HADOOP_CONF_DIR</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_PREFIX</span><span class="k">}</span>/etc/hadoop
<span class="nb">export </span><span class="nv">YARN_CONF_DIR</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_PREFIX</span><span class="k">}</span>/etc/hadoop
<span class="nb">export </span><span class="nv">HDFS_CONF_DIR</span><span class="o">=</span><span class="k">${</span><span class="nv">HADOOP_PREFIX</span><span class="k">}</span>/etc/hadoop
<span class="nb">export </span><span class="nv">HADOOP_COMMON_LIB_NATIVE_DIR</span><span class="o">=</span><span class="nv">$HADOOP_HOME</span>/lib/native
<span class="nb">export </span><span class="nv">HADOOP_OPTS</span><span class="o">=</span><span class="s2">"-Djava.library.path=</span><span class="nv">$HADOOP_HOME</span><span class="s2">/lib"</span>
</code></pre></div></div>

<p>修改好后执行 <code class="language-plaintext highlighter-rouge">source ~/.bashcr</code> 命令使环境变量生效。</p>

<p><strong>Note</strong>: Hadoop CDH 版本的所有配置文件都在其压缩包的 etc/hadoop 目录下。</p>

<h3 id="配置-slaves-即运行-datanode-和-nodemanager-服务的节点">配置 slaves (即运行 DataNode 和 NodeManager 服务的节点)</h3>

<p>只需要在 $HADOOP_CONF_DIR/slaves 文件中加入节点的 ip 即可。每个 ip 占一行。</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>172.16.2.141
172.16.2.142
172.16.2.143
172.16.2.144
172.16.2.145
</code></pre></div></div>

<p>我把5台机器都作为 DataNode 和 MapReduce 的计算节点来用了。</p>

<h3 id="修改-hadoop-envsh">修改 hadoop-env.sh</h3>

<p>將其中的 JAVA_HOME 变量修改成 JAVA 安装目录的绝对路径，用全局环境变量的 JAVA_HOME 执行 MapReduce 应用有时会出问题。</p>

<p><strong>NOTE</strong>: 要理解下面配置中各个 properties 的意义，请先阅读 <a href="http://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/HDFSHighAvailabilityWithQJM.html">HDFS High Availability Using the Quorum Journal Manager</a> 一文，我就不逐一讲解了。</p>

<h3 id="配置-core-sitexml">配置 core-site.xml</h3>

<p>在 core-site.xml 文件中加入以下 properties：</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>fs.defaultFS<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>hdfs://hadoopcluster<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>ha.zookeeper.quorum<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>172.16.2.141:2181，172.16.2.142:2181,172.16.2.143:2181<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div></div>

<h3 id="配置-hdfs-sitexml">配置 hdfs-site.xml</h3>

<p>在 hdfs-site.xml 文件中加入以下 properties:</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>dfs.namenode.name.dir<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/home/hadoop/data/namenode<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.datanode.data.dir<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>/home/hadoop/data/datanode<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>dfs.namenode.handler.count<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>100<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.blocksize<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>67108864<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.nameservices<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>hadoopcluster<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.ha.namenodes.hadoopcluster<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>nn1,nn2<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>dfs.namenode.rpc-address.hadoopcluster.nn1<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>172.16.2.141:8020<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>dfs.namenode.rpc-address.hadoopcluster.nn2<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>172.16.2.142:8020<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>dfs.namenode.shared.edits.dir<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>qjournal://172.16.2.141:8485;172.16.2.142:8485;172.16.2.143:8485/hadoopcluster<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>dfs.journalnode.edits.dir<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>/home/clouder/data/journalnode/data<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>dfs.client.failover.proxy.provider.weather<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>dfs.ha.fencing.methods<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>shell(/home/hadoop/fencingscript.sh)<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>dfs.ha.automatic-failover.enabled<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>dfs.replication<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>3<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div></div>

<p>然后在所有机器上创建 <code class="language-plaintext highlighter-rouge">dfs.namenode.name.dir</code>、<code class="language-plaintext highlighter-rouge">dfs.datanode.data.dir</code> 和 <code class="language-plaintext highlighter-rouge">dfs.journalnode.edits.dir</code> 所指定的目录。</p>

<h3 id="创建-fencing-script">创建 fencing script</h3>

<p>將以下脚本内容保存为 fencingscript.sh 脚本，修改执行权限后，将其复制到运行 NameNode 服务的两台机器上的 $HOME 目录里（也即 hdfs-site.xml 配置的 <code class="language-plaintext highlighter-rouge">dfs.ha.fencing.methods</code> 属性所设置的目录）。</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c">#!/bin/bash</span>

<span class="nv">isNNEmpty</span><span class="o">=</span><span class="sb">`</span>jps | <span class="nb">grep </span>NameNode<span class="sb">`</span>
<span class="k">if</span> <span class="o">[</span> <span class="s2">"X</span><span class="k">${</span><span class="nv">isNNEmpty</span><span class="k">}</span><span class="s2">"</span> <span class="o">=</span> <span class="s2">"X"</span> <span class="o">]</span><span class="p">;</span> <span class="k">then</span>
    <span class="k">${</span><span class="nv">HADOOP_HOME</span><span class="k">}</span>/sbin/hadoop-daemon.sh start namenode
<span class="k">fi
</span><span class="nb">exit </span>0
</code></pre></div></div>

<p>这个脚本主要是在两台 NameNode 的其中一台宕机，可以自动重启宕机的 NameNode。</p>

<h3 id="配置-mapred-sitexml">配置 mapred-site.xml</h3>

<p>本方案使用的是 yarn 框架，所以配置如下：</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>mapreduce.framework.name<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>yarn<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>

<span class="nt">&lt;property&gt;</span>
  <span class="nt">&lt;name&gt;</span>mapreduce.shuffle.port<span class="nt">&lt;/name&gt;</span>
  <span class="nt">&lt;value&gt;</span>8888<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div></div>

<h3 id="配置-yarn-sitexml">配置 yarn-site.xml</h3>

<p>YARN 的 ResourceManager 配置在172.16.2.143機器上，所以配置如下：</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.address<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>172.16.2.143:8032<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.scheduler.address<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>172.16.2.143:8030<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.webapp.address<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>172.16.2.143:8088<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.admin.address<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>172.16.2.143:8033<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.resourcemanager.resource-tracker.address<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>172.16.2.143:8031<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
     <span class="nt">&lt;name&gt;</span>yarn.nodemanager.aux-services<span class="nt">&lt;/name&gt;</span>
     <span class="nt">&lt;value&gt;</span>mapreduce.shuffle<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div></div>

<p>配置完成後，將配置文件同步到整個集群，需要進行一些初始化工作。</p>

<h3 id="初始化-zookeeper">初始化 ZooKeeper</h3>

<p>在两个NameNode中的其中一个的$HADOOP_HOME/bin目录里执行以下命令：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hdfs zkfc <span class="nt">-formatZK</span>
</code></pre></div></div>

<p>觀察 Terminal 輸出，確保沒有 error。</p>

<h3 id="启动-journalnode">启动 JournalNode</h3>

<p>分别在 clou01、clou02、clou03 三台机器的 $HADOOP_HOME/sbin 目录下执行以下命令来启动 JournalNode 进程：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hadoop-daemon.sh start journalnode
</code></pre></div></div>

<h3 id="格式化-namenode">格式化 NameNode</h3>

<p>在其中一个 NameNode 的 $HADOOP_HOME/bin 目录里执行以下命令来格式化 NameNode：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hadoop namenode <span class="nt">-format</span>
</code></pre></div></div>

<p>然后复制该 NameNode 的 <code class="language-plaintext highlighter-rouge">dfs.namenode.name.dir</code> 目录的数据到另外一个 NameNode 的同一目录中。</p>

<h3 id="启动-hdfs">启动 HDFS</h3>

<p>在任意一个 NameNode 的 sbin 目录下执行 <code class="language-plaintext highlighter-rouge">./start-dfs.sh</code> 命令来启动 HDFS。</p>

<h3 id="启动-yarn">启动 YARN</h3>

<p>在 ResourceManager 角色的机器（这里是172.16.2.143）的 $HADOOP_HOME/sbin 目录下执行 <code class="language-plaintext highlighter-rouge">start-yarn.sh</code> 命令来启动 YARN。</p>

<h3 id="测试-1">测试</h3>

<p>在浏览器地址栏中输入下表地址，可查看对应的 Hadoop 角色的状态 web 页面。</p>

<ul>
  <li>172.16.2.141:50070  NameNode</li>
  <li>172.16.2.142:50070  NameNode</li>
  <li>172.16.2.141:8088   ResourceManager</li>
</ul>

<h2 id="部署-hbase">部署 HBase</h2>

<h3 id="解压-hbase-压缩包">解压 HBase 压缩包</h3>

<h3 id="配置-regionservers">配置 regionservers</h3>

<p>编辑 $HBASE_HOME/conf/regionservers 文件，添加运行 region server 服务的机器 ip，每个一行。</p>

<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>172.16.2.143
172.16.2.144
172.16.2.145
</code></pre></div></div>

<h3 id="配置-hbase-envsh">配置 hbase-env.sh</h3>

<ol>
  <li>设置 JAVA_HOME 变量，使其为机器的 JAVA_HOME 绝对路径。</li>
  <li>设置 HBASE_CLASSPATH，使其为 $HADOOP_CONF_DIR，因为 HBase 要找到 HDFS 的配置。</li>
  <li>设置 HBASE_PID_DIR，使其为某本地非临时文件夹。如果保持默认的话，HBase 集群会不能正常关闭。</li>
</ol>

<h3 id="配置-hbase-sitexml">配置 hbase-site.xml</h3>

<p>编辑 conf/hbase-site.xml 文件，添加以下属性：</p>

<div class="language-xml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>hbase.rootdir<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>hdfs://hadoopcluster/hbase<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>hbase.cluster.distributed<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>true<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.zookeeper.quorum<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>172.16.2.141,172.16.2.142,172.16.2.143<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
    <span class="nt">&lt;name&gt;</span>hbase.zookeeper.property.dir<span class="nt">&lt;/name&gt;</span>
    <span class="nt">&lt;value&gt;</span>/home/hadoop/data/tmp/zookeeper<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>hbase.tmp.dir<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>/home/hadoop/data/hbase/tmp<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
<span class="nt">&lt;property&gt;</span>
   <span class="nt">&lt;name&gt;</span>hbase.regionserver.handler.count<span class="nt">&lt;/name&gt;</span>
   <span class="nt">&lt;value&gt;</span>100<span class="nt">&lt;/value&gt;</span>
<span class="nt">&lt;/property&gt;</span>
</code></pre></div></div>

<p>在每台机器上创建 habse.tmp.dir 目录。</p>

<h3 id="启动-hbase">启动 HBase</h3>

<p>ssh 登录到 active NameNode 所在节点，运行 <code class="language-plaintext highlighter-rouge">start-hbase.sh</code> 脚本启动 HBase 集群。</p>

<h3 id="测试-2">测试</h3>

<p>在浏览器中输入 <code class="language-plaintext highlighter-rouge">&lt;active NameNode ip&gt;:60010</code>，查看 HBase 集群的运行状态。</p>

<p>-EOF-</p>

  </article>
</div>

    <footer class="relative px-4 pt-4 pb-4">
  <div class="flex flex-wrap items-center justify-center">
    <div class="text-sm text-gray-300">
      &copy; 2023 Hugo Chou
    </div>
  </div>
</footer>

  </body>
</html>
